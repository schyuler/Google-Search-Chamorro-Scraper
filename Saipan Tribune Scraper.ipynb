{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ae8ccf1-bb77-4e79-9da9-ca0038289ce8",
   "metadata": {},
   "source": [
    "# Web Scraper for Saipan Tribune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531705e-863d-448c-a7fe-dcb780976742",
   "metadata": {},
   "source": [
    "## About This Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79440504-f396-4aea-a8f3-6657215dca4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2234ed53-6c12-46c5-8503-9d4316ad6b1d",
   "metadata": {},
   "source": [
    "**Name:** Schyuler Lujan <br>\n",
    "**Date Started:** 23-April-2025 <br>\n",
    "**Date Completed:** In Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095da1a-4c88-4508-bb02-e6ee02f923aa",
   "metadata": {},
   "source": [
    "## Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa5c9f9-875d-4b0c-9371-71c584149c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (if not already installed)\n",
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa8a6aa4-64bc-4610-9dda-bb32cebae73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a731120-d99a-4090-8a6e-edd42ae151a6",
   "metadata": {},
   "source": [
    "## Get page navigation links for search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bd44600-d254-4f81-9a05-c144a2b0c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set url for initial search on website\n",
    "initial_search_url = 'https://www.saipantribune.com/index.php/author/johnsdelrosariojr/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e78e6-acbe-4f26-afa0-5dfd561d764c",
   "metadata": {},
   "source": [
    "From looking on the website, we know that searching on the author John S Del Rosario Jr produces 216 pages of results. We will create a list of links that includes all of these page results.\n",
    "\n",
    "Future Edits: This function will be modified to dynamically capture the total page results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f68e1e17-faab-4cd6-8abb-2d67bd19144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_search_urls(initial_search):\n",
    "    \"\"\"\n",
    "    Returns a list of all the URLS we will use to search on the Saipan Tribune website.\n",
    "\n",
    "    Args:\n",
    "        initial_search (str): The first url we will start our search on\n",
    "\n",
    "    Returns:\n",
    "        all_search_urls (list): A list of all the urls we will search on\n",
    "    \"\"\"\n",
    "    # Initialize list\n",
    "    search_urls = [initial_search]\n",
    "\n",
    "    for i in range(1,217):\n",
    "        next_page = initial_search + f\"page/{i}/\"\n",
    "        search_urls.append(next_page)\n",
    "\n",
    "    return search_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98eee336-de4c-4149-96c2-a32f67668721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all search URLs\n",
    "search_urls = create_all_search_urls(initial_search_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50963c00-ec39-46dc-b791-521342dbcc0e",
   "metadata": {},
   "source": [
    "## Get Links to Each Article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16aeff-ffd9-45e7-a1f5-ed918871ddd1",
   "metadata": {},
   "source": [
    "In this section, we will navigate to each seach page and use `BeautifulSoup` to get the links to each article that appears in the search. After examining the structure of the search results webpages, the links to the blogs are contained in `<a href>` tags under `<h2>` tags where `class=\"blog-title\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "935a2984-b904-4b95-a7b4-1497aefbfd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_urls(all_search_urls):\n",
    "    \"\"\"\n",
    "    Navigates to all the search urls and uses BeautifulSoup to parse each page and get the individual article links.\n",
    "    \n",
    "    Args:\n",
    "        all_search_urls: A list of the urls, which link to the webpages that hold the individual article links\n",
    "\n",
    "    Returns:\n",
    "        article_urls: A list of the urls to each individual news article\n",
    "    \"\"\"\n",
    "    # Initialize list to store article URLs\n",
    "    article_urls = []\n",
    "\n",
    "    # Navigate to each search result page and get the URLs for each article on the page\n",
    "    for url in all_search_urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Raise error for bad responses\n",
    "            response.encoding = response.apparent_encoding\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find the individual article links\n",
    "            articles = soup.find_all(\"h2\", class_=\"blog-title\")\n",
    "\n",
    "            # Extract the article links\n",
    "            for article in articles:\n",
    "                article_link = article.find(\"a\")\n",
    "                article_urls.append(article_link[\"href\"])\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "\n",
    "        # Pause between searches\n",
    "        time.sleep(1)\n",
    "            \n",
    "    return article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e460a3c5-034b-4948-b00f-cff898aba46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get article links\n",
    "article_links = get_article_urls(search_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9541dbaf-8ece-45f8-93ff-9875d660338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIXME: Remove duplicates in the function\n",
    "article_links = list(set(article_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de50b3f5-3e20-4c44-b771-ffe46989e0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "862\n"
     ]
    }
   ],
   "source": [
    "# View link count\n",
    "print(len(article_links))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b38cf-2375-4016-9d43-c80f6c246472",
   "metadata": {},
   "source": [
    "## Detect English Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6af2e5-8869-4cce-a229-cbfdb56c955f",
   "metadata": {},
   "source": [
    "In this section, we will write a function that allows us to detect if content is written in English. This will allow us to exclude articles that we do not want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "909e0ac9-3d70-4cfb-8dc5-08e310edf0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Implement a function that returns True if the text is in English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85614d4a-df67-4eb3-b863-3194355bf37a",
   "metadata": {},
   "source": [
    "## Scrape Article Metadata and Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ceac7-ba15-4962-a720-83816a72a4be",
   "metadata": {},
   "source": [
    "We will use `BeautifulSoup` to scrape the text content from the hyperlinks. After examining the elements of some articles on the website, these are the following classes we will be targeting for scraping:\n",
    "\n",
    "* **blog title:** `class_=\"blog-title\"`\n",
    "* **blog author:** `\"div\", class_=\"blog-author\"`\n",
    "* **blog date:** `\"div\", class_=\"blog-date\"`\n",
    "* **blog content:** `div\", class_=\"blog-content\"`\n",
    "\n",
    "Note for blog content: This is the body text fo the article, and is sometimes nested under `div` in paragraph tags, and other times it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07504b12-82c9-40c7-be64-2cffd0de2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(urls):\n",
    "    \"\"\"\n",
    "    Navigates to each article url, scrapes the text content and returns results in a dictionary.\n",
    "\n",
    "    Args:\n",
    "        urls: A unique list of URLs for each article\n",
    "\n",
    "    Returns:\n",
    "        contents: A dci\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store scraped data\n",
    "    contents = {}\n",
    "    \n",
    "    # Initialize counter for naming convention in contents\n",
    "    counter = 0\n",
    "    \n",
    "    # Go to each webpage and parse it\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Raise error for bad responses\n",
    "            response.encoding = response.apparent_encoding\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find the blog-title, blog-author, blog-date and blog-content\n",
    "            title = soup.find(class_=\"blog-title\")\n",
    "            author = soup.find(\"div\", class_=\"blog-author\")\n",
    "            date = soup.find(\"div\", class_=\"blog-date\")\n",
    "            content_div = soup.find(\"div\", class_=\"blog-content\")\n",
    "            \n",
    "            # If elements are found for metadata, convert to strings, otherwise return N/A\n",
    "            title = title.get_text(strip=True) if title else \"N/A\"\n",
    "            author = author.get_text() if author else \"N/A\"\n",
    "            date = date.get_text() if date else \"N/A\"\n",
    "            \n",
    "            # Extract the text content; some content is nested in <p> tags, some aren't\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all(\"p\")\n",
    "                if paragraphs:\n",
    "                    content = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "                else:\n",
    "                    content = content_div.get_text(strip=True) if content_div else \"N/A\"\n",
    "\n",
    "            # Add article metadata and text to contents\n",
    "            counter += 1\n",
    "            contents[f\"article_{counter}\"] = {\n",
    "                \"title\":title, \n",
    "                \"author\":author, \n",
    "                \"date\":date, \n",
    "                \"url\":str(url), \n",
    "                \"text\":content\n",
    "            }\n",
    "    \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "\n",
    "        time.sleep(1)\n",
    "              \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "177492b5-f689-45c4-ab6f-1b9e009e887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.saipantribune.com/index.php/%ef%bb%bfmafabrikan-kuttura/: 404 Client Error: Not Found for url: https://www.saipantribune.com/index.php/%EF%BB%BFmafabrikan-kuttura/\n"
     ]
    }
   ],
   "source": [
    "# Scrape \n",
    "contents = get_contents(article_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66ccea-2a5f-4d5b-8977-1af845586f85",
   "metadata": {},
   "source": [
    "### Export Data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9060f1a4-7fc6-42fe-b6cd-adbdc9ba1996",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('saipan_tribune_delrosario_john.json', 'w', encoding=\"utf-8\") as f:\n",
    "    json.dump(contents, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adac4d-2f7f-4459-b12a-6e5466531c4e",
   "metadata": {},
   "source": [
    "## Export Articles to HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903bdf45-2900-42fa-ab0d-1756286268ac",
   "metadata": {},
   "source": [
    "TO-DO: Add an \"about\" for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "de1771fb-e76f-400c-8574-f3b31fb45f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content(blog_contents, blog_title=\"Saipan Tribune\"):\n",
    "    \"\"\"\n",
    "    Iterates through the scraped blog content, creates an HTML structure for formatting, appends blog content to this\n",
    "    structure, and returns the combined HTML content.\n",
    "    \"\"\"\n",
    "    # Initialize HTML structure for formatting\n",
    "    combined_html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head><meta charset = \"UTF-8\"><title>{blog_title}</title></head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "    for article in blog_contents:\n",
    "        # Get the article metadata and text contents\n",
    "        title = blog_contents[article]['title']\n",
    "        author = blog_contents[article]['author']\n",
    "        date = blog_contents[article]['date']\n",
    "        url = blog_contents[article]['url']\n",
    "        text = blog_contents[article]['text']\n",
    "        \n",
    "        # Convert text with \\n\\n into HTML <p> paragraphs\n",
    "        paragraphs = ''.join(f\"<p>{para.strip()}</p>\\n\" for para in text.split('\\n\\n') if para.strip())\n",
    "        \n",
    "        # Append the content to the HTML structure\n",
    "        combined_html_content += f\"\"\"\n",
    "        <section>\n",
    "        <h1>{title}</h1>\n",
    "        <p><strong>Date:</strong> {date}<p>\n",
    "        <p><strong>Author:</strong> {author}<p>\n",
    "        <p><strong>URL:</strong> {url}<p>\n",
    "        {paragraphs}\n",
    "        </section>\n",
    "        <hr>\n",
    "        \"\"\"\n",
    "    # Close the HTML structure\n",
    "    combined_html_content += f\"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return combined_html_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a94aa2-776c-40e5-9a5d-88467f33b78f",
   "metadata": {},
   "source": [
    "### Export Contents to an HTML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "73d00cf9-5b31-456e-a5b2-7c5bfa426711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export contents to an HTML File\n",
    "# with open(\"saipantribune_test.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b774c7b-d72d-4a6f-ae65-2a0360d5701e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
