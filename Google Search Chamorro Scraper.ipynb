{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee304be0",
   "metadata": {},
   "source": [
    "# Google Search Chamorro Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704150b6",
   "metadata": {},
   "source": [
    "**About:** This project is meant for educational purposes, to scrape and export articles written in the Chamorro language from the Saipan Tribune news website, where the search for those articles is narrowed down via a Google search by searching for a common Chamorro stopword in the text body. We will be using the Google Custom Search JSON API to find and scrape content. The text results will be processed in two ways: \n",
    "\n",
    "1. The full text exported into an HTML file for conversion to other reader-friendly formats, such as PDF, .DOCX, or .EPUB\n",
    "2. A unique word list will be exported to .CSV for additional analysis and integration into other learning tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74086b",
   "metadata": {},
   "source": [
    "**Name:** Schyuler Lujan <br>\n",
    "**Date Started:** 23-April-2025 <br>\n",
    "**Date Completed:** In Progress <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74fdfe",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97fa7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa57bb0",
   "metadata": {},
   "source": [
    "## Call The API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95c66d",
   "metadata": {},
   "source": [
    "### Set the API Key, CSE ID, and Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949c0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key and Custom Search Engine ID\n",
    "API_KEY = 'AIzaSyBv-v2ToEeizr_7l4xSRr5IzpbzWH8Jiz4' # API Key goes here\n",
    "CSE_ID = '2157f0d44bb5d4139' #custom search engine ID goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6964d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set search query to a common Chamorro stopword on the target website\n",
    "QUERY = 'allintext: na site:https://www.saipantribune.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd084bf2",
   "metadata": {},
   "source": [
    "### Calculate the Values for the `start` Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a955bcb",
   "metadata": {},
   "source": [
    "When we call the API, we need to specify which results we want it to return to us by setting the `start` parameter to the appropriate value. When setting the `start` parameter to the appropriate value, we will keep the following in mind:\n",
    "\n",
    "* A Google search returns 10 results per page\n",
    "* Setting the `start` parameter to 1 returns the first page of results\n",
    "* To get the results on subsequent pages, we must increase the start parameter by 10 each time\n",
    "\n",
    "To get all the results of our search, we will do the following:\n",
    "\n",
    "1. Return the total number of pages that our search returns and store that value in the variable `total_pages`\n",
    "2. Calculate the start values based on `total_pages`, and store those values in a list. We will calculate a range to do this: `list(range(1,total_pages,10))`\n",
    "\n",
    "**Free Tier Limits:** We are also using the *free tier* of the Custom Search API, which allows up to 100 queries per day. To be mindful of this limit, we will also split our start ranges into lists that return a maximum of 100 results each, and run them on separate days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c9ee4",
   "metadata": {},
   "source": [
    "### Make the Request and Return Total Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77de8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query, api_key, cse_id, start=1):\n",
    "    \"\"\"\n",
    "    Sends the search query to Google by calling the Google Custom Search JSON API.\n",
    "    Returns the results in a dictionary (JSON format).\n",
    "    \"\"\"\n",
    "    # Set the endpoint for Google's search API\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    \n",
    "    # Set the query parameters to send to the API\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cse_id,\n",
    "        'q': query,\n",
    "        'start': start\n",
    "    }\n",
    "    \n",
    "    # Make the HTTP GET request to Google\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84d50c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make request\n",
    "search_results = google_search(QUERY, API_KEY, CSE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5701be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Results:  1630\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of results\n",
    "total_results = int(search_results.get(\"searchInformation\", {}).get(\"totalResults\", 0))\n",
    "print(\"Total Results: \", total_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94f487",
   "metadata": {},
   "source": [
    "### Get Links to 100 Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2634bd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to a maximum of 100 results\n",
    "max_results = min(total_results, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "126d26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set list for storing links\n",
    "links = []\n",
    "\n",
    "# Loop over pages in increments of 10, get results, and store hyperlink for each item in links\n",
    "for start in range(1, max_results + 1, 10):\n",
    "    results = google_search(QUERY, API_KEY, CSE_ID, start=start)\n",
    "    items = results.get(\"items\", [])\n",
    "    \n",
    "    # Get the link for the content and store in links\n",
    "    for item in items:\n",
    "        links.append(str(item[\"link\"]))\n",
    "    \n",
    "    # One-second pause between requests\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b326454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "https://www.saipantribune.com/index.php/bdaa937c-1dfb-11e4-aedf-250bc8c9958e/\n"
     ]
    }
   ],
   "source": [
    "print(len(links))\n",
    "print(links[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20241aed",
   "metadata": {},
   "source": [
    "## Scrape Text Content From URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acd898",
   "metadata": {},
   "source": [
    "We will use `BeautifulSoup` to scrape the text content from the hyperlinks. After examining the elements of some articles on the website, these are the following classes we will be targeting for scraping:\n",
    "\n",
    "* **blog title: **`class_=\"blog-title\"`\n",
    "* **blog author: **`\"div\", class_=\"blog-author\"`\n",
    "* **blog date: **`\"div\", class_=\"blog-date\"`\n",
    "* **blog content: **`div\", class_=\"blog-content\"`\n",
    "\n",
    "Blog content is sometimes nested under `div` in paragraph tags, and other times it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bf045e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(urls):\n",
    "    \"\"\"\n",
    "    Iterates through the list of hyperlinks from google_search, scrapes the text content and returns results in a dictionary\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store scraped data\n",
    "    contents = {}\n",
    "    \n",
    "    # Initialize counter for naming convention in contents\n",
    "    counter = 0\n",
    "    \n",
    "    # Go to each webpage and parse it\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Raise error for bad responses\n",
    "            response.encoding = response.apparent_encoding #\"utf-8\"\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find the blog-title, blog-author, blog-date and blog-content\n",
    "            title = soup.find(class_=\"blog-title\")\n",
    "            author = soup.find(\"div\", class_=\"blog-author\")\n",
    "            date = soup.find(\"div\", class_=\"blog-date\")\n",
    "            content_div = soup.find(\"div\", class_=\"blog-content\")\n",
    "            \n",
    "            # If elements are found, convert to strings, otherwise return N/A\n",
    "            title = title.get_text(strip=True) if title else \"N/A\"\n",
    "            author = author.get_text(strip=True) if author else \"N/A\"\n",
    "            date = date.get_text(strip=True) if date else \"N/A\"\n",
    "            \n",
    "            # Find text content; some articles have text content nested in <p> tags under <div>\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all(\"p\")\n",
    "                if paragraphs:\n",
    "                    content = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "                else:\n",
    "                    content = content_div.get_text(strip=True) if content_div else \"N/A\"\n",
    "\n",
    "            # Add data to contents\n",
    "            counter += 1 # For naming\n",
    "            contents[f\"article_{counter}\"] = {\n",
    "                \"title\":title, \n",
    "                \"author\":author, \n",
    "                \"date\":date, \n",
    "                \"url\":str(url), \n",
    "                \"text\":content\n",
    "            }\n",
    "    \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "              \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2eb866bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_1': {'title': 'Diskalentau Na Kinalamten',\n",
       "  'author': 'By',\n",
       "  'date': 'Posted onNov 27 2011',\n",
       "  'url': 'https://www.saipantribune.com/index.php/bdaa937c-1dfb-11e4-aedf-250bc8c9958e/',\n",
       "  'text': 'Estaba un’ palaoan gi entalu’ filan estante gi un’ tenda guine gi alacha. Sige ha akompara presiun fektus siha. Ha atan hulu’ ya ilegña, “Magpu’ i nobena, Iku. Mas kahulu’ i presiun nesessidat familia siha ya sige ha’ man-ma’aña hit ni surcharge yan asuterity”. Mamahlauyu’ sa’ sumasaunauyu’ umesalau “let it be”. Pues hu ñañgon i prohima na “tana fan salakumba’ i manpairen Hegsu’ I Deni” gi otru sakan. Konfotme.\\n\\n—\\n\\n\\n\\nMientras ha a’ayeg mas fektus, ilegña, “Ke lau hafa na mampos bahu kinalamten tano’ta Iku na biahe? Hafa adai bidan niniha i man-ma’gasta na no siakassu siha nu i baban linala’ i publiku lau siha timuneru?”\\n\\nIneppe as Magoo, “Man-yayas ya ni siha ti matuñgu’ hafa para umachogue”. Ginen fuetsau na pinadese, mas ke dos mit na natibu dumiñgu i tanu’. Taya’ alibiu guine para disente na linala’ familia. Tatnai sisida este siha na pinadese lau tano’ta mismu mafa’ sasalaguan natibu. De dios mihu!\\n\\n—\\n\\nHumanau un abugau mamokat gi fihon pantalan Susupe. ‘Nai dumeskansa, man-arima i lalahita ya sige masañgan dineskontentun niha nu i gobietno. Ilegña i prohimu, “Ti hu tuñgu’ na taiguine minigaiña ti gumoflamen i maga’lahe”.\\n\\nBuente chagu’ i hegsu’ i Deni na makat hoñgga i kabales na sentimenton publiku. Lau ‘an mattu otru sakan na eleksion, un’ ratutu ha’ anog, klaru sin ditension, hafa sentimentoña i mampos mamadedese na publiku. Ti u lache este ya sumen libianu machalegua’.\\n\\n—\\n\\nGi mapos na Huebes (Thanksgivig Day) hunae’ gi hechuran tinayuyothu agradesimientu i dos sinantusan na sainahu pot todu sakrifision niha pot hame. Ha tampe si nanahu pinadesen mame gi un’ didog na pineble ni sabanas hinimidde, pinasensia yan mañan silensiu. Ti umuguñg ni unabes lau motmut tinayuyotña para bai’n fan gai esperansa na i haane u homhum antes de manana.\\n\\n—\\n\\nHu e’ekuñgog dinaña’ estudianten NMC ni man-manae’ asuntu para u prepara para presentasion. I liñguahen, i kinemprenden niha yan eskalera ‘nai man-a’adiñgan sumen na’magodai sa’ makomprende i asuntu. Edukau na konbetsasion gi entre siha. Klaru na todu fina’ koyentura maguadog hulu ginen tinaitai niha siha leblo yan materiat.\\n\\nHafa na man-siña este siha na estuduante “edukau” konbetsasion niha? Sa’ man-eskuekuela ya guaha dañkulu na animu pot para umakomprende leksion niha. Sumen namagof ya ginen este na gumuaha konfiansaghu na esta man-mamamaila’ i ta eduduka na famaguonta para u chahlau i talen responsablidat ginen i man-ma’gasta ni mampos man-fatigau.\\n\\n—\\n\\nI tautauta ti man-seha fumatta ginen korason niha i sensian tradision Katoliku. Maseha man-mamatmos gi halom didog na fache’ chinatsaga, ti mahñau man-mamahan sentadan pabu gi mapos na semana. Metgot na sensian kilisyanu i para u mana’ guaha dinaña familia ni matutuhon finenina gi tinayuyot. Sumen na’ banidosu na sensian komunida.\\n\\n—\\n\\nGuaha mamaisen hafa tanu’ gobietnu yan publiku. Todu tanu’ guine ni ti praibet pues tanu’ publiku. Todu i ha chule’ i gobietnu para uson publiku, debi dispues u basiha ennau na tanu’ guatu gi depattamentun tanu (DPL). I mamaneha, usu yan disposision tanu’ publiku gaige ennau na atoridat konstitusion gi depattamentun tanu’. Tai atoridat i gobietnu guine na patte. Uson tanu publiku mas ke siñku hektarias, debi u inina yan inaprueba nu i lehislatura. Pues i priniponen ma-usan tanu’ sankatan siha na islas, debi u falofan ya u maina nu i lehislatura.\\n\\n—\\n\\nYangin hayumu hosguan pot suette i otru, pues ennau na sensiamu para un’ linimus gi halom tasen minala’et. Memegaiña gi ha’animu lastima sa’ mampos hau champada ‘nai adumidide’ maliñgu dignomu yan sensian hinimidde. Fanayuyot ya un’ manae’ motmut mina’agim. Kontodu siempre siñientemu kulan mana’ suha trosun lulog ni hagas humogse hau.\\n\\n'},\n",
       " 'article_2': {'title': 'Probleman Sanhalom',\n",
       "  'author': ',ByJohn S. Del Rosario Jr.',\n",
       "  'date': 'Posted OnJun 04 2015 06:00',\n",
       "  'url': 'https://www.saipantribune.com/index.php/tag/fiet-na-gacho/',\n",
       "  'text': 'Ta fafana` te`ug na problema pot appenas moneda gi kahan iya Marianas. Estague` mina` makat ta apase obligasionta siha kontodu dibin CUC. I umatisa este gai tinatte: Mahuchum fakterian magagu yan hinanau bisnis man-dañkulu siha na bisnis Japones. Ha bali huyung poku `mas o`menus...'}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DELETE ME TEST scraping content on two articles; one with nested content, one without nested content ###\n",
    "test_url = links[2:4]\n",
    "get_content(test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840d473",
   "metadata": {},
   "source": [
    "## Export Full Text To HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b21655",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIXME: Export the text to a formatted HTML file ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958b2b3",
   "metadata": {},
   "source": [
    "## Create and Export a Unique Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIXME: Clean text, split text into words, remove duplicates and export to CSV ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec4373",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ace29f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST Calculate the start values for getting all the results ###\n",
    "#start_values = list(range(1, total_results, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ea038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View search results\n",
    "# for item in search_results.get(\"items\", []):\n",
    "#     print(item[\"link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CODE: Iterating through multiple stopwords when querying the website (not as feasible on free tier) ###\n",
    "# # Set the address for the website that will be queried\n",
    "# site = 'site:https://www.saipantribune.com/'\n",
    "\n",
    "# # List common Chamorro stopwords to be used in the query\n",
    "# common_stopwords = ['na', 'yan', 'ya', 'nu', 'ni', 'gi', 'ti']\n",
    "\n",
    "# # Create a list of the queries\n",
    "# QUERY = [word + \" \" + site for word in common_stopwords]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
