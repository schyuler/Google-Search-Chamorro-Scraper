{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee304be0",
   "metadata": {},
   "source": [
    "# Google Search Chamorro Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704150b6",
   "metadata": {},
   "source": [
    "**About:** This project is meant for educational purposes, to scrape and export articles written in the Chamorro language from the Saipan Tribune news website, where the search for those articles is narrowed down via a Google search by searching for a common Chamorro stopword in the text body. We will be using the Google Custom Search JSON API to find and scrape content. The text results will be processed in two ways: \n",
    "\n",
    "1. The full text exported into an HTML file for conversion to other reader-friendly formats, such as PDF, .DOCX, or .EPUB\n",
    "2. A unique word list will be exported to .CSV for additional analysis and integration into other learning tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a74086b",
   "metadata": {},
   "source": [
    "**Name:** Schyuler Lujan <br>\n",
    "**Date Started:** 23-April-2025 <br>\n",
    "**Date Completed:** In Progress <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74fdfe",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97fa7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa57bb0",
   "metadata": {},
   "source": [
    "## Call The API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95c66d",
   "metadata": {},
   "source": [
    "### Set the API Key, CSE ID, and Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "949c0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your API Key and Custom Search Engine ID\n",
    "API_KEY = # API Key goes here\n",
    "CSE_ID = #custom search engine ID goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f6964d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set search query to a common Chamorro stopword on the target website\n",
    "QUERY = 'allintext: na site:https://www.saipantribune.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd084bf2",
   "metadata": {},
   "source": [
    "### Calculate the Values for the `start` Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a955bcb",
   "metadata": {},
   "source": [
    "When we call the API, we need to specify which results we want it to return to us by setting the `start` parameter to the appropriate value. When setting the `start` parameter to the appropriate value, we will keep the following in mind:\n",
    "\n",
    "* A Google search returns 10 results per page\n",
    "* Setting the `start` parameter to 1 returns the first page of results\n",
    "* To get the results on subsequent pages, we must increase the start parameter by 10 each time\n",
    "\n",
    "To get all the results of our search, we will do the following:\n",
    "\n",
    "1. Return the total number of pages that our search returns and store that value in the variable `total_pages`\n",
    "2. Calculate the start values based on `total_pages`, and store those values in a list. We will calculate a range to do this: `list(range(1,total_pages,10))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c9ee4",
   "metadata": {},
   "source": [
    "### Make the Request and Return Total Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "77de8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_search(query, api_key, cse_id, start=1):\n",
    "    \"\"\"\n",
    "    Sends the search query to Google by calling the Google Custom Search JSON API.\n",
    "    Returns the results in a dictionary (JSON format).\n",
    "    \"\"\"\n",
    "    # Set the endpoint for Google's search API\n",
    "    url = 'https://www.googleapis.com/customsearch/v1'\n",
    "    \n",
    "    # Set the query parameters to send to the API\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cse_id,\n",
    "        'q': query,\n",
    "        'start': start\n",
    "    }\n",
    "    \n",
    "    # Make the HTTP GET request to Google\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84d50c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make request\n",
    "search_results = google_search(QUERY, API_KEY, CSE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "002e8a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Results:  1630\n"
     ]
    }
   ],
   "source": [
    "# Get the total number of results\n",
    "total_results = int(search_results.get(\"searchInformation\", {}).get(\"totalResults\", 0))\n",
    "print(\"Total Results: \", total_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc8412",
   "metadata": {},
   "source": [
    "### Get Links to 100 Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b12ad8",
   "metadata": {},
   "source": [
    "**Free Tier Limits:** We are also using the *free tier* of the Custom Search API, which allows up to 100 queries per day. To be mindful of this limit, we will also split our start ranges into lists that return a maximum of 100 results each, and run them on separate days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "506da64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(search_results):\n",
    "    \"\"\"\n",
    "    Gets the first 100 links from the google search results and returns a list of links.\n",
    "    \"\"\"\n",
    "    # Set list for storing links\n",
    "    links = []\n",
    "    \n",
    "    # Limit to a maximum of 100 results\n",
    "    max_results = min(search_results, 100)\n",
    "\n",
    "    # Loop over pages in increments of 10, get results, and store hyperlink for each item in links\n",
    "    for start in range(1, max_results + 1, 10):\n",
    "        results = google_search(QUERY, API_KEY, CSE_ID, start=start)\n",
    "        items = results.get(\"items\", [])\n",
    "\n",
    "        # Get the link for the content and store in links\n",
    "        for item in items:\n",
    "            links.append(str(item[\"link\"]))\n",
    "\n",
    "        # One-second pause between requests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0293a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get links from the google search results\n",
    "all_links = get_links(total_results)\n",
    "# DELETE ME TEST CODE Get a slice of the links for testing\n",
    "test_urls = all_links[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20241aed",
   "metadata": {},
   "source": [
    "## Scrape Text Content From URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16a563b",
   "metadata": {},
   "source": [
    "We will use `BeautifulSoup` to scrape the text content from the hyperlinks. After examining the elements of some articles on the website, these are the following classes we will be targeting for scraping:\n",
    "\n",
    "* **blog title:** `class_=\"blog-title\"`\n",
    "* **blog author:** `\"div\", class_=\"blog-author\"`\n",
    "* **blog date:** `\"div\", class_=\"blog-date\"`\n",
    "* **blog content:** `div\", class_=\"blog-content\"`\n",
    "\n",
    "Note for blog content: This is the body text fo the article, and is sometimes nested under `div` in paragraph tags, and other times it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf045e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(urls):\n",
    "    \"\"\"\n",
    "    Iterates through the list of hyperlinks from google_search, scrapes the text content and returns results in a dictionary\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store scraped data\n",
    "    contents = {}\n",
    "    \n",
    "    # Initialize counter for naming convention in contents\n",
    "    counter = 0\n",
    "    \n",
    "    # Go to each webpage and parse it\n",
    "    for url in urls:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status() # Raise error for bad responses\n",
    "            response.encoding = response.apparent_encoding #\"utf-8\"\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            # Find the blog-title, blog-author, blog-date and blog-content\n",
    "            title = soup.find(class_=\"blog-title\")\n",
    "            author = soup.find(\"div\", class_=\"blog-author\")\n",
    "            date = soup.find(\"div\", class_=\"blog-date\")\n",
    "            content_div = soup.find(\"div\", class_=\"blog-content\")\n",
    "            \n",
    "            # If elements are found, convert to strings, otherwise return N/A\n",
    "            title = title.get_text(strip=True) if title else \"N/A\"\n",
    "            author = author.get_text(strip=True) if author else \"N/A\"\n",
    "            date = date.get_text(strip=True) if date else \"N/A\"\n",
    "            \n",
    "            # Extract the text content; some content is nested in <p> tags, some aren't\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all(\"p\")\n",
    "                if paragraphs:\n",
    "                    content = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "                else:\n",
    "                    content = content_div.get_text(strip=True) if content_div else \"N/A\"\n",
    "\n",
    "            # Add data to contents\n",
    "            counter += 1 # For naming in the dictionary\n",
    "            contents[f\"article_{counter}\"] = {\n",
    "                \"title\":title, \n",
    "                \"author\":author, \n",
    "                \"date\":date, \n",
    "                \"url\":str(url), \n",
    "                \"text\":content\n",
    "            }\n",
    "    \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {url}: {e}\")\n",
    "              \n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7b840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DELETE ME TEST CODE for testing HTML formatting and file export###\n",
    "contents = get_contents(test_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e840d473",
   "metadata": {},
   "source": [
    "## Export Full Text To HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21b21655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content(blog_contents, blog_title=\"Saipan Tribune\"):\n",
    "    \"\"\"\n",
    "    Iterates through the scraped blog content, creates an HTML structure for formatting, appends blog content to this\n",
    "    structure, and returns the combined HTML content.\n",
    "    \"\"\"\n",
    "    # Initialize HTML structure for formatting\n",
    "    combined_html_content = f\"\"\"\n",
    "    <html>\n",
    "    <head><meta charset = \"UTF-8\"><title>{blog_title}</title></head>\n",
    "    <body>\n",
    "    \"\"\"\n",
    "    for article in blog_contents:\n",
    "        # Get the article metadata and text contents\n",
    "        title = blog_contents[article]['title']\n",
    "        author = blog_contents[article]['author']\n",
    "        date = blog_contents[article]['date']\n",
    "        url = blog_contents[article]['url']\n",
    "        text = blog_contents[article]['text']\n",
    "        \n",
    "        # Convert text with \\n\\n into HTML <p> paragraphs\n",
    "        paragraphs = ''.join(f\"<p>{para.strip()}</p>\\n\" for para in text.split('\\n\\n') if para.strip())\n",
    "        \n",
    "        # Append the content to the HTML structure\n",
    "        combined_html_content += f\"\"\"\n",
    "        <section>\n",
    "        <h1>{title}</h1>\n",
    "        <p><strong>Date:</strong> {date}<p>\n",
    "        <p><strong>Author:</strong> {author}<p>\n",
    "        <p><strong>URL:</strong> {url}<p>\n",
    "        {paragraphs}\n",
    "        </section>\n",
    "        <hr>\n",
    "        \"\"\"\n",
    "    # Close the HTML structure\n",
    "    combined_html_content += f\"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    return combined_html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bf5be4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export scraped content to an HTML structure\n",
    "html_content = format_content(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdbf98e",
   "metadata": {},
   "source": [
    "### Export Contents to an HTML File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db65afbc",
   "metadata": {},
   "source": [
    "Export `html_content` to an HTML file, which can then be converted to different formats that are easier for reading, such as .epub, .PDF, .docx, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "99a90dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export contents to an HTML File\n",
    "with open(\"saipantribune_test.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958b2b3",
   "metadata": {},
   "source": [
    "## Create and Export a Unique Word List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307a966",
   "metadata": {},
   "source": [
    "The data will also be used to increase the Chamorro Lexicon, by adding any new or missing words into the online dictionary maintained at https://www.lengguahita.com/. To do this, we will get a unique word count from the text contents of the articles and return .csv file that contains a list of those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c5d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIXME: Clean text, split text into words, remove duplicates and export to CSV ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec4373",
   "metadata": {},
   "source": [
    "## Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5dfd1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST Calculate the start values for getting all the results ###\n",
    "#start_values = list(range(1, total_results, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ea038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View search results\n",
    "# for item in search_results.get(\"items\", []):\n",
    "#     print(item[\"link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CODE: Iterating through multiple stopwords when querying the website (not as feasible on free tier) ###\n",
    "# # Set the address for the website that will be queried\n",
    "# site = 'site:https://www.saipantribune.com/'\n",
    "\n",
    "# # List common Chamorro stopwords to be used in the query\n",
    "# common_stopwords = ['na', 'yan', 'ya', 'nu', 'ni', 'gi', 'ti']\n",
    "\n",
    "# # Create a list of the queries\n",
    "# QUERY = [word + \" \" + site for word in common_stopwords]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
